# This is a simple chat completion service

## Requirements

### LLM Model

- 1B-7B model (approx. 2GB-16GB memory)
- Run on 1 GPU (cloud GPU)
- Handles basic text completion/chat
- Low latency (sub-second responses for short prompts)
- Fits into budget cloud GPU (â‰¤ $0.50/hr)

Search models on https://huggingface.co:

- [LLaMA-2-7B](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
- [gemma3:270m](https://huggingface.co/google/gemma-3-270m) (in use)

### Model serving engines

- Supports NVIDIA ([CUDA](https://developer.nvidia.com/cuda-toolkit))
- Easy to set up
- API integration, simple API access

Engines:

- slim [Ollama](https://github.com/alpine-docker/ollama) (in use)
- [vLLM](https://docs.vllm.ai/en/latest/)

## HOWTO

### Deploy model/engine to EKS

1. Env vars:

```
export AWS_PROFILE=ak-dev
```

```
export AWS_REGION=us-east-1
```

```
export EKS_NAME=j-llm
```

2. Update kubeconfig

```
aws eks update-kubeconfig --name ${EKS_NAME} --region ${AWS_REGION} --profile ${AWS_PROFILE} --alias ${EKS_NAME}
```

3. Chose cluster context

```
kubectl config use-context j-llm
```

4. Deploy Ollama

```
kubectl apply -f deploy
```

### Monitoring (move to infra)

```
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -f values.yaml -n j-llm
```

### Local test

1. Pull the gemma3:270m model

```
curl http://localhost:11434/api/pull -d '{
  "model": "gemma3:270m",
}'
```

(Optional) Ollama API, Generate completion

```
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

3. Install open-webui

```
docker run -d \
  -p 3030:8080 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  -v open-webui:/app/backend/data \
  --name open-webui \
  --restart always \
  ghcr.io/open-webui/open-webui:main
```
